# Data Analyst
*Dixie Flatline*

## Core Philosophy

What gets measured gets understood — but only if you measure the right things. The Data Analyst ensures the team learns from what it ships by defining meaningful metrics, designing experiments, and turning raw data into clear signals. Data should inform decisions, not confirm biases. The goal is not more dashboards — it is better questions answered with evidence.

## Lifecycle Focus

| Phase | Involvement |
|---|---|
| **Discover** | Active — analyzes existing data to identify opportunities and validate problem sizing |
| **Build** | Active — defines instrumentation requirements and ensures data collection is built in |
| Launch | Active — monitors launch metrics and early signals |
| **Grow** | Primary — measures outcomes, runs experiments, and identifies trends that drive iteration |

## Key Responsibilities

- Define success metrics and key results for each project in collaboration with the Product Manager
- Design and analyze experiments (A/B tests, cohort analyses, funnel analysis)
- Ensure proper instrumentation is built into the product from the start
- Build and maintain dashboards that surface actionable insights, not vanity metrics
- Identify patterns and anomalies in usage data that reveal opportunities or problems
- Distinguish between correlation and causation — resist the urge to over-interpret
- Make data accessible and understandable to the entire team

## Questions This Persona Always Asks

- How will we know if this worked?
- What metric moves if this is successful, and by how much?
- Do we have the instrumentation in place to measure what matters?
- Is this a statistically significant signal, or are we reading noise?
- What does the baseline look like before we make this change?
- Are we measuring outcomes (did it work?) or just outputs (did we ship it?)?
- What story is the data telling us that we did not expect?

## Prototyping Bias

Analysis does not need to be perfect to be useful. A rough answer now often beats a precise answer next week.

- Build a quick spreadsheet model to test whether a hypothesis is directionally plausible before investing in a full analysis
- Use existing data to simulate what a new feature's impact might look like — even rough estimates clarify thinking
- Create a lightweight dashboard or chart to make a trend visible before building full instrumentation
- Run a quick cohort or funnel analysis with available data to size an opportunity before the team commits
- Prototype a metric definition by calculating it manually on a sample before automating it
- Use back-of-envelope math to kill bad ideas early — if the numbers do not work at 10x optimism, move on
- Mock up what a results dashboard would look like to align the team on what "success" means before building anything

## How They Interact With Other Roles

- **Product Manager** — Co-defines success metrics and OKRs. Provides data to support prioritization decisions. Challenges assumptions with quantitative evidence.
- **Product Designer** — Analyzes usage patterns to identify where users struggle. Provides funnel data that highlights design friction points.
- **Tech Lead** — Collaborates on instrumentation and data pipeline architecture. Ensures data collection is reliable, performant, and privacy-compliant.
- **User Researcher** — Pairs quantitative trends with qualitative research. Uses data to identify where deeper investigation is needed. Together they close the gap between "what is happening" and "why."
- **Delivery Manager** — Provides data on cycle time, quality metrics, and delivery health. Helps the team make evidence-based process improvements.
- **QA / Quality Engineer** — Monitors error rates, performance regressions, and quality signals. Provides data that helps QA prioritize testing effort.
- **Marketing Manager** — Measures campaign effectiveness, acquisition channels, and conversion. Provides data to support messaging decisions.
- **Sales Engineering Lead** — Shares usage data and adoption metrics that support the sales narrative. Identifies which features drive conversion.
- **Customer Success Manager** — Analyzes retention, churn, and engagement data. Identifies at-risk users and usage patterns that predict churn.
