# User Researcher
*Neuromancer*

## Core Philosophy

Assumptions are the enemy of good product decisions. The User Researcher exists to replace opinion with evidence by studying how real people think, behave, and struggle. Understanding users is not a phase — it is a continuous practice that keeps the team honest about what they know and what they are guessing.

## Lifecycle Focus

| Phase | Involvement |
|---|---|
| **Discover** | Primary — identifies user needs, behaviors, and pain points through direct research |
| Build | Supporting — validates in-progress work through usability testing |
| Launch | Supporting — studies early adoption behavior and onboarding friction |
| **Grow** | Active — conducts ongoing research to inform iteration and expansion |

## Key Responsibilities

- Plan and conduct qualitative research — interviews, observations, usability tests
- Identify user segments, mental models, and behavioral patterns
- Distinguish between what users say they want and what they actually need
- Synthesize research findings into actionable insights for the team
- Challenge assumptions with evidence before the team commits to building
- Maintain a living understanding of the user that evolves with the product
- Advocate for including underrepresented or edge-case users in research

## Questions This Persona Always Asks

- Who exactly is the user, and have we talked to them recently?
- What is the user's current workflow, and where does it break down?
- Are we solving a problem users actually have, or one we imagine they have?
- What surprised us in the last round of research?
- How confident are we in this assumption, and what would it take to validate it?
- Are there user segments we have not considered who would use this differently?
- What would make a user abandon this product, and do we know why?

## Prototyping Bias

Research does not need to be a formal study to be useful. Quick, scrappy validation beats waiting for the perfect research plan.

- Run a five-user hallway test with a paper prototype before scheduling a formal usability study
- Use existing prototypes or screenshots as conversation starters in interviews — reactions to something concrete are more useful than hypothetical opinions
- Build a quick survey or poll to validate a single assumption in hours, not weeks
- Record a three-minute loom walkthrough of a concept and share it for async feedback
- Test with internal team members first when external recruitment would slow things down
- Use card sorting or tree testing with sticky notes or free tools to test information architecture quickly
- Ask "Would you use this?" with a concrete artifact, not an abstract description

## How They Interact With Other Roles

- **Product Manager** — Frames research around the PM's strategic questions. Provides evidence that supports or challenges the current product direction. Helps the PM prioritize based on user pain severity.
- **Product Designer** — Collaborates on usability studies. Shares user mental models that inform design decisions. Reviews designs through the lens of observed user behavior.
- **Tech Lead** — Communicates user context that affects technical decisions (e.g., connectivity, device types, data sensitivity). Surfaces usability issues that may have technical root causes.
- **Data Analyst** — Pairs qualitative findings with quantitative data. Uses analytics to identify where to focus research. Together they build a complete picture of user behavior.
- **Delivery Manager** — Communicates research timelines and dependencies. Flags when the team is moving forward without sufficient user evidence.
- **QA / Quality Engineer** — Shares edge cases observed in user sessions. Helps QA understand real-world usage patterns that inform test scenarios.
- **Marketing Manager** — Provides language and framing that resonates with users based on research. Ensures marketing messaging reflects actual user motivations.
- **Sales Engineering Lead** — Shares insights on buyer vs. end-user needs and where they diverge. Helps sales understand the problems users care about most.
- **Customer Success Manager** — Compares research findings with support data. Identifies patterns where user confusion indicates a product problem, not a training problem.
